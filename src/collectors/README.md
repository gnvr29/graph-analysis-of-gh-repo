# GitHub Data Pipeline (Issues & PRs) para Neo4j

Este projeto √© um pipeline de dados robusto projetado para extrair **Issues** (Abertas e Fechadas) e **Pull Requests** de um reposit√≥rio GitHub (neste caso, focado no `streamlit/streamlit`) e ingeri-los em um banco de dados Neo4j.

O principal objetivo √© construir um grafo de conhecimento das atividades do reposit√≥rio, capturando n√£o apenas os itens principais, mas tamb√©m suas intera√ß√µes (coment√°rios, revis√µes) e metadados (autores, datas, status).

O pipeline √© constru√≠do para ser **resiliente** e **eficiente em termos de mem√≥ria**, utilizando uma estrat√©gia de *streaming* (com geradores Python) para processar um item de cada vez, permitindo a coleta de reposit√≥rios massivos sem estourar a mem√≥ria RAM.

## üèóÔ∏è Como Funciona (Arquitetura)

O pipeline √© orquestrado pelo `main.py` e opera em um fluxo de *streaming* (item por item):

1.  **Inicializa√ß√£o:** O `main.py` √© executado.
2.  **Conex√£o:** Ele primeiro estabelece uma conex√£o com o banco de dados Neo4j usando o `Neo4jService`.
3.  **Coleta de Issues:**
    * O `GithubCollector` √© iniciado.
    * O `main.py` chama `github_collector.collect_issues()` (abertas) e `collect_closed_issues()` (fechadas).
    * Esses m√©todos s√£o **geradores (`yield`)**. Eles n√£o baixam tudo de uma vez.
    * Para cada issue retornada pelo gerador, o `main.py` imediatamente a envia para o `neo4j_service.insert_issue_data()`.
4.  **Coleta de Pull Requests:**
    * O `main.py` chama `github_collector.collect_all_pull_requests_api()`.
    * Este m√©todo tamb√©m √© um gerador que usa a **API do GitHub**.
    * Para cada PR retornado, o `main.py` o envia para `neo4j_service.insert_pull_request_data()`.
5.  **Finaliza√ß√£o:** Ap√≥s a conclus√£o de todos os fluxos (ou interrup√ß√£o), a conex√£o com o Neo4j √© fechada.

## ‚ú® Estrat√©gias de Coleta e Recursos

Este coletor utiliza duas estrat√©gias diferentes para maximizar a coleta de dados e contornar os limites da API.

### 1. Issues: Estrat√©gia H√≠brida (Scraping + API Fallback)

Para evitar o r√°pido esgotamento da cota da API do GitHub (que √© limitada), a coleta de issues prioriza o *scraping* das p√°ginas p√∫blicas.

* **Scraping (Prim√°rio):**
    * O `github_collector.py` usa `requests.Session` para baixar a p√°gina HTML das issues (ex: `/streamlit/streamlit/issues`).
    * O `github_parser.py` (`html_to_json`) extrai um bloco JSON gigante embutido na p√°gina (um `<script data-target="react-app.embeddedData">`).
    * As fun√ß√µes `interpretar_issues` e `interpretar_comentarios` navegam por esse JSON complexo para extrair os dados da issue, corpo e coment√°rios.
* **API Fallback (Secund√°rio):**
    * O scraping do GitHub pode falhar. O `_collect_paginated_issues` detecta essa falha, para o *loop* de scraping e aciona o `collect_issues_api`.
    * Este m√©todo de API (`collect_issues_api`) assume a coleta a partir da p√°gina onde o scraping parou, garantindo que nenhum dado seja perdido.

### 2. Pull Requests: Estrat√©gia Pura (API)

A coleta de Pull Requests √© feita **exclusivamente pela API v3 do GitHub**. Isso √© necess√°rio para obter dados detalhados que n√£o est√£o facilmente dispon√≠veis via scraping, como:

* Status (`merged`, `closed`, `open`)
* Datas (`createdAt`, `closedAt`, `mergedAt`)
* Revis√µes (`reviews`)
* Coment√°rios de Revis√£o (em linhas de c√≥digo)
* Coment√°rios da Issue (na thread principal do PR)

### 3. Efici√™ncia e Resili√™ncia

* **Streaming (Geradores `yield`):** A mem√≥ria permanece baixa, pois apenas um item (issue ou PR) √© mantido em mem√≥ria por vez.
* **Controle de Rate Limit:** O coletor monitora o cabe√ßalho `X-RateLimit-Remaining` da API. Se o limite estiver baixo (< 50), ele pausa automaticamente por 60 segundos.
* **Toler√¢ncia a Falhas:** O pipeline pode ser interrompido a qualquer momento com `Ctrl+C`. Como os dados s√£o inseridos item por item, tudo o que foi coletado at√© aquele momento j√° est√° salvo no Neo4j.

## üóÇÔ∏è Estrutura do Projeto

* `main.py`: O orquestrador do pipeline. Ponto de entrada para iniciar a coleta.
* `src/collectors/github_collector.py`: O c√©rebro da opera√ß√£o. Cont√©m a classe `GithubCollector` com toda a l√≥gica de scraping, chamadas de API, pagina√ß√£o e o *fallback*.
* `src/parsers/github_parser.py`: Fun√ß√µes auxiliares respons√°veis por "traduzir" o HTML/JSON bruto (obtido pelo scraper) em dicion√°rios Python limpos.
* `src/services/neo4j_service.py`: **(Arquivo Faltante)** Este arquivo √© essencial. Ele deve conter a classe `Neo4jService` com a l√≥gica de conex√£o ao banco e os m√©todos:
    * `insert_issue_data(issue_dict)`
    * `insert_pull_request_data(pr_dict)`
* `config/settings.py`: **(Arquivo Faltante)** Arquivo que carrega as vari√°veis de ambiente do `.env`.
* `.env`: **(Arquivo Faltante)** Arquivo para armazenar credenciais e segredos.

---

## üöÄ Como Configurar e Rodar

### 1. Pr√©-requisitos

* Python 3.8+
* Uma inst√¢ncia do Neo4j (local ou em nuvem)
* Um **Token de Acesso Pessoal (Classic)** do GitHub.
    * √â **essencial** para a coleta de PRs e para o fallback da API.
    * Crie um em: `GitHub > Settings > Developer settings > Personal access tokens (Classic)`.
    * Marque apenas o escopo `public_repo`.

### 2. Instala√ß√£o

1.  Clone este reposit√≥rio.
2.  Crie um ambiente virtual:
    ```bash
    python -m venv venv
    source venv/bin/activate 
    # ou "venv\Scripts\activate" no Windows
    ```
3.  Instale as depend√™ncias. Crie um arquivo `requirements.txt`:
    ```txt
    # requirements.txt
    requests
    beautifulsoup4
    neo4j
    python-dotenv
    ```
    E instale:
    ```bash
    pip install -r requirements.txt
    ```

### 3. Arquivos de Configura√ß√£o (Essencial)

Voc√™ **precisa** criar os dois arquivos faltantes para o projeto funcionar.

**A. Crie o arquivo `.env`:**

Crie um arquivo chamado `.env` na raiz do projeto. Ele **nunca** deve ser enviado ao GitHub (adicione-o ao `.gitignore`).

```ini
# .env
# --- Credenciais Neo4j ---
NEO4J_URI="neo4j://localhost:7687"
NEO4J_USERNAME="neo4j"
NEO4J_PASSWORD="sua_senha_aqui"

# --- Token da API do GitHub ---
GITHUB_API_TOKEN="seu_token_aqui_ghp_..."# GitHub Data Pipeline (Issues & PRs) para Neo4j

Este projeto √© um pipeline de dados robusto projetado para extrair **Issues** (Abertas e Fechadas) e **Pull Requests** de um reposit√≥rio GitHub (neste caso, focado no `streamlit/streamlit`) e ingeri-los em um banco de dados Neo4j.

O principal objetivo √© construir um grafo de conhecimento das atividades do reposit√≥rio, capturando n√£o apenas os itens principais, mas tamb√©m suas intera√ß√µes (coment√°rios, revis√µes) e metadados (autores, datas, status).

O pipeline √© constru√≠do para ser **resiliente** e **eficiente em termos de mem√≥ria**, utilizando uma estrat√©gia de *streaming* (com geradores Python) para processar um item de cada vez, permitindo a coleta de reposit√≥rios massivos sem estourar a mem√≥ria RAM.

## üèóÔ∏è Como Funciona (Arquitetura)

O pipeline √© orquestrado pelo `main.py` e opera em um fluxo de *streaming* (item por item):

1.  **Inicializa√ß√£o:** O `main.py` √© executado.
2.  **Conex√£o:** Ele primeiro estabelece uma conex√£o com o banco de dados Neo4j usando o `Neo4jService`.
3.  **Coleta de Issues:**
    * O `GithubCollector` √© iniciado.
    * O `main.py` chama `github_collector.collect_issues()` (abertas) e `collect_closed_issues()` (fechadas).
    * Esses m√©todos s√£o **geradores (`yield`)**. Eles n√£o baixam tudo de uma vez.
    * Para cada issue retornada pelo gerador, o `main.py` imediatamente a envia para o `neo4j_service.insert_issue_data()`.
4.  **Coleta de Pull Requests:**
    * O `main.py` chama `github_collector.collect_all_pull_requests_api()`.
    * Este m√©todo tamb√©m √© um gerador que usa a **API do GitHub**.
    * Para cada PR retornado, o `main.py` o envia para `neo4j_service.insert_pull_request_data()`.
5.  **Finaliza√ß√£o:** Ap√≥s a conclus√£o de todos os fluxos (ou interrup√ß√£o), a conex√£o com o Neo4j √© fechada.

## ‚ú® Estrat√©gias de Coleta e Recursos

Este coletor utiliza duas estrat√©gias diferentes para maximizar a coleta de dados e contornar os limites da API.

### 1. Issues: Estrat√©gia H√≠brida (Scraping + API Fallback)

Para evitar o r√°pido esgotamento da cota da API do GitHub (que √© limitada), a coleta de issues prioriza o *scraping* das p√°ginas p√∫blicas.

* **Scraping (Prim√°rio):**
    * O `github_collector.py` usa `requests.Session` para baixar a p√°gina HTML das issues (ex: `/streamlit/streamlit/issues`).
    * O `github_parser.py` (`html_to_json`) extrai um bloco JSON gigante embutido na p√°gina (um `<script data-target="react-app.embeddedData">`).
    * As fun√ß√µes `interpretar_issues` e `interpretar_comentarios` navegam por esse JSON complexo para extrair os dados da issue, corpo e coment√°rios.
* **API Fallback (Secund√°rio):**
    * O scraping do GitHub pode falhar. O `_collect_paginated_issues` detecta essa falha, para o *loop* de scraping e aciona o `collect_issues_api`.
    * Este m√©todo de API (`collect_issues_api`) assume a coleta a partir da p√°gina onde o scraping parou, garantindo que nenhum dado seja perdido.

### 2. Pull Requests: Estrat√©gia Pura (API)

A coleta de Pull Requests √© feita **exclusivamente pela API v3 do GitHub**. Isso √© necess√°rio para obter dados detalhados que n√£o est√£o facilmente dispon√≠veis via scraping, como:

* Status (`merged`, `closed`, `open`)
* Datas (`createdAt`, `closedAt`, `mergedAt`)
* Revis√µes (`reviews`)
* Coment√°rios de Revis√£o (em linhas de c√≥digo)
* Coment√°rios da Issue (na thread principal do PR)

### 3. Efici√™ncia e Resili√™ncia

* **Streaming (Geradores `yield`):** A mem√≥ria permanece baixa, pois apenas um item (issue ou PR) √© mantido em mem√≥ria por vez.
* **Controle de Rate Limit:** O coletor monitora o cabe√ßalho `X-RateLimit-Remaining` da API. Se o limite estiver baixo (< 50), ele pausa automaticamente por 60 segundos.
* **Toler√¢ncia a Falhas:** O pipeline pode ser interrompido a qualquer momento com `Ctrl+C`. Como os dados s√£o inseridos item por item, tudo o que foi coletado at√© aquele momento j√° est√° salvo no Neo4j.

## üóÇÔ∏è Estrutura do Projeto

* `main.py`: O orquestrador do pipeline. Ponto de entrada para iniciar a coleta.
* `src/collectors/github_collector.py`: O c√©rebro da opera√ß√£o. Cont√©m a classe `GithubCollector` com toda a l√≥gica de scraping, chamadas de API, pagina√ß√£o e o *fallback*.
* `src/parsers/github_parser.py`: Fun√ß√µes auxiliares respons√°veis por "traduzir" o HTML/JSON bruto (obtido pelo scraper) em dicion√°rios Python limpos.
* `src/services/neo4j_service.py`: Este arquivo √© essencial. Ele cont√äM a classe `Neo4jService` com a l√≥gica de conex√£o ao banco e os m√©todos:
    * `insert_issue_data(issue_dict)`
    * `insert_pull_request_data(pr_dict)`
* `config/settings.py`: Arquivo que carrega as vari√°veis de ambiente do `.env`.
* `.env`: Arquivo para armazenar credenciais e segredos.

---

## üöÄ Como Configurar e Rodar

### 1. Pr√©-requisitos

* Python 3.8+
* Uma inst√¢ncia do Neo4j (local ou em nuvem)
* Um **Token de Acesso Pessoal (Classic)** do GitHub.
    * √â **essencial** para a coleta de PRs e para o fallback da API.
    * Crie um em: `GitHub > Settings > Developer settings > Personal access tokens (Classic)`.
    * Marque apenas o escopo `public_repo`.

### 2. Instala√ß√£o

1.  Clone este reposit√≥rio.
2.  Crie um ambiente virtual:
    ```bash
    python -m venv venv
    source venv/bin/activate 
    # ou "venv\Scripts\activate" no Windows
    ```
3.  Instale as depend√™ncias. Crie um arquivo `requirements.txt`:
    ```txt
    # requirements.txt
    requests
    beautifulsoup4
    neo4j
    python-dotenv
    ```
    E instale:
    ```bash
    pip install -r requirements.txt
    ```

### 3. Arquivos de Configura√ß√£o (Essencial)

Voc√™ **precisa** criar o arquivo .env faltante para o projeto funcionar.

**A. Crie o arquivo `.env`:**

Crie um arquivo chamado `.env` na raiz do projeto. Ele **nunca** deve ser enviado ao GitHub (adicione-o ao `.gitignore`).

```ini
# .env
# --- Credenciais Neo4j ---
NEO4J_URI="neo4j://localhost:7687"
NEO4J_USERNAME="neo4j"
NEO4J_PASSWORD="sua_senha_aqui"

# --- Token da API do GitHub ---
GITHUB_API_TOKEN="seu_token_aqui_ghp_..."